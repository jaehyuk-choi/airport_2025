# -*- coding: utf-8 -*-
"""
Airport - Preprocessing
- _was_imputed í”Œë˜ê·¸ ì™„ì „ ì œê±°
- ì´ìƒì¹˜(0/ìŒìˆ˜/ë¡¤ë§-IQR/MAD) â†’ NaN ë§ˆìŠ¤í‚¹ í›„ ë³´ê°„
- ê³ ê¸‰ ë³´ê°„ (yearÃ—monthÃ—weekday â†’ monthÃ—weekday â†’ month)
- ë‹¤ì¤‘ê³µì„ ì„± ì œê±°, RF feature selection
- ìƒì„¸ ì½˜ì†” ë¦¬í¬íŠ¸
"""

import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor


# =========================
# ìœ í‹¸
# =========================
def winsorize(s: pd.Series, lower=0.005, upper=0.995):
    if s.isna().all():
        return s
    lo, hi = s.quantile(lower), s.quantile(upper)
    return s.clip(lower=lo, upper=hi)

def contiguous_blocks(idx: pd.DatetimeIndex, max_gap_days=1):
    if len(idx) == 0:
        return []
    diffs = idx.to_series().diff().dt.days.fillna(1).astype(int)
    block_ids = (diffs > max_gap_days).cumsum()
    return [sub.index for _, sub in idx.to_series().groupby(block_ids)]

def add_lag_and_roll_safe(df: pd.DataFrame, target="Vehicle_Max",
                          lags=(1,7,14), roll_windows=(7,14)) -> pd.DataFrame:
    out = df.copy()
    if target not in out.columns:
        return out
    for block_idx in contiguous_blocks(out.index, max_gap_days=1):
        sub = out.loc[block_idx, [target]].copy()
        for lag in lags:
            out.loc[block_idx, f"{target}_lag_{lag}"] = sub[target].shift(lag)
        for w in roll_windows:
            out.loc[block_idx, f"{target}_roll_mean_{w}"] = sub[target].shift(1).rolling(w, min_periods=1).mean()
            out.loc[block_idx, f"{target}_roll_std_{w}"]  = sub[target].shift(1).rolling(w, min_periods=1).std()
    return out

def consecutive_nan_runs(s: pd.Series):
    isna = s.isna().astype(int)
    grp = (isna.ne(isna.shift())).cumsum()
    runs = isna.groupby(grp).sum()
    return runs[runs > 0].tolist()


# =========================
# 0) ê¸°ë³¸ ì „ì²˜ë¦¬(ë³´ê°„/ì´ìƒì¹˜ ì „)
# =========================
def preprocess_basics(df: pd.DataFrame,
                           start_date="2014-01-01",
                           end_date="2025-04-30") -> pd.DataFrame:
    before_cols = df.columns.tolist()
    df.drop(columns=['exit_count', 'entry_count'], errors='ignore', inplace=True)
    # datetime ë³µì› + ì—°ë„ í•„í„°
    if "datetime" in df.columns:
        df["datetime"] = pd.to_datetime(df["datetime"], errors="coerce")
    else:
        df["datetime"] = pd.to_datetime(df[["Year","Month","Day"]], errors="coerce")
    df = df[(df["datetime"] >= pd.Timestamp(start_date)) &
            (df["datetime"] <= pd.Timestamp(end_date))].copy()
    df.sort_values("datetime", inplace=True)

    # íƒ€ê¹ƒ í†µì¼
    if "Vehicle_Max_New" in df.columns and "Vehicle_Max" not in df.columns:
        df["Vehicle_Max"] = df["Vehicle_Max_New"]

    # íœ´ì¼ ê²°ì¸¡ ê¸°ë³¸ê°’
    for col in ["Holiday","Holiday_Type", "Holiday_Pattern", "Holiday_Pattern_2"]:
        if col in df.columns:
            df[col] = df[col].fillna("N")

    # ì‚­ì œ(ëˆ„ì¶œ/ì¤‘ë³µ/ì¡ìŒ)
    drop_cols = [
        "Year","Month","Day","Vehicle_Max_New",
        "Holiday_Encoding","Is_Holiday",
        # "1_0_Encoding",
        "Unnamed: 28","Unnamed: 27",
        "Tobit_Upper_Limit","Vehicle_Tobit","Vehicle_Tobit_Adjusted",
        "Up_Down",
    ]
    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True, errors="ignore")

    # ê³µí•­/ì¸ë±ìŠ¤
    df["airport"] = "jeju"
    df.set_index("datetime", inplace=True)

    # ê°•ìˆ˜ëŸ‰ ê²°ì¸¡ 0
    if "Precip_mm" in df.columns:
        df["Precip_mm"] = df["Precip_mm"].fillna(0)

    # ìº˜ë¦°ë” íŒŒìƒ
    if "Day_of_the_Week" in df.columns:
        dow_raw = df["Day_of_the_Week"].astype(float)
        df["Day_sin"] = np.sin(2*np.pi*dow_raw/7.0)
        df["Day_cos"] = np.cos(2*np.pi*dow_raw/7.0)
        df.drop(columns=["Day_of_the_Week"], inplace=True, errors="ignore")
    else:
        dow = df.index.weekday
        df["Day_sin"] = np.sin(2*np.pi*dow/7.0)
        df["Day_cos"] = np.cos(2*np.pi*dow/7.0)

    df["month"] = df.index.month
    df["doy"]   = df.index.dayofyear
    df["month_sin"] = np.sin(2*np.pi*df["month"]/12.0)
    df["month_cos"] = np.cos(2*np.pi*df["month"]/12.0)
    df["doy_sin"]   = np.sin(2*np.pi*df["doy"]/365.25)
    df["doy_cos"]   = np.cos(2*np.pi*df["doy"]/365.25)
    df["dow"] = df.index.weekday
    df["is_weekend"] = (df["dow"] >= 5).astype(int)

    # íƒ€ê¹ƒ ì•ˆì •í™”
    if "Vehicle_Max" not in df.columns:
        raise ValueError("íƒ€ê¹ƒ 'Vehicle_Max'ê°€ ì—†ìŠµë‹ˆë‹¤.")
    df["Vehicle_Max"] = winsorize(df["Vehicle_Max"], 0.005, 0.995).clip(lower=0)

    # time_idx
    min_date = df.index.min()
    df["time_idx"] = (df.index - min_date).days.astype(int)

    # ë¦¬í¬íŠ¸
    after_cols = df.reset_index().columns.tolist()
    kept   = [c for c in before_cols if c in after_cols]
    dropped= [c for c in before_cols if c not in after_cols]
    added  = [c for c in after_cols if c not in before_cols]

    print("\n[1/6] ì»¬ëŸ¼ ë³€í™” ìš”ì•½ (23~25 í•„í„° ì ìš©)")
    print(f"- ì…ë ¥ ì¹¼ëŸ¼ ìˆ˜: {len(before_cols)}")
    print(f"- ì¶œë ¥ ì¹¼ëŸ¼ ìˆ˜: {len(after_cols)}  (datetime í¬í•¨)")
    print("  Â· ì‚­ì œ:", sorted(dropped))
    print("  Â· ìœ ì§€:", sorted(kept))
    print("  Â· ì¶”ê°€:", sorted(added))
    print(f"  Â· ë‚ ì§œ ë²”ìœ„: {df.index.min().date()} ~ {df.index.max().date()}  (ì´ {len(df):,}ì¼)")

    return df


# =========================
# 1) ì´ìƒì¹˜ ë§ˆìŠ¤í‚¹ (â†’ NaN)
# =========================
def detect_outliers_rolling_iqr(s: pd.Series, window=5, k=2.5, past_only=True):
    """
    ë¡¤ë§ IQR ê¸°ë°˜ ì´ìƒì¹˜(True = ì´ìƒì¹˜)
    - past_only=Trueë©´ í˜„ì¬ê°’ íŒë‹¨ì— 'ê³¼ê±°'ë§Œ ì‚¬ìš©(shift(1))
    """
    base = s.shift(1) if past_only else s
    q1 = base.rolling(window, min_periods=max(5, window//3)).quantile(0.25)
    q3 = base.rolling(window, min_periods=max(5, window//3)).quantile(0.75)
    iqr = q3 - q1
    low = q1 - k*iqr
    high = q3 + k*iqr
    return (s < low) | (s > high)

def detect_outliers_rolling_mad(s: pd.Series, window=35, z=5.0, past_only=True):
    """
    ë¡¤ë§ Median + MAD ê¸°ë°˜ ì´ìƒì¹˜(True = ì´ìƒì¹˜)
    """
    base = s.shift(1) if past_only else s
    med = base.rolling(window, min_periods=max(5, window//3)).median()
    mad = (base - med).abs().rolling(window, min_periods=max(5, window//3)).median()
    robust_z = 0.6745 * (s - med) / (mad.replace(0, np.nan))
    return robust_z.abs() > z

def mask_outliers(df: pd.DataFrame,
                  zero_outlier_cols=None,
                  iqr_cols=None,
                  mad_cols=None,
                  iqr_window=35, iqr_k=2.5,
                  mad_window=35, mad_z=5.0,
                  past_only=True):
    """
    - zero_outlier_cols: 0/ìŒìˆ˜ â†’ NaN (counts/strict-positiveì— ê¶Œì¥)
    - iqr_cols: ë¡¤ë§ IQR ì´ìƒì¹˜ â†’ NaN
    - mad_cols: ë¡¤ë§ MAD ì´ìƒì¹˜ â†’ NaN
    """
    zero_outlier_cols = zero_outlier_cols or []
    iqr_cols = iqr_cols or []
    mad_cols = mad_cols or []

    print("\n[2/6] ì´ìƒì¹˜ ë§ˆìŠ¤í‚¹ ìš”ì•½")
    for c in zero_outlier_cols:
        if c not in df.columns: 
            continue
        n0 = int((df[c] == 0).sum())
        nn = int((df[c] < 0).sum())
        if n0 or nn:
            df.loc[df[c] <= 0, c] = np.nan
        print(f" - {c}: 0ê°’ {n0}ê±´, ìŒìˆ˜ {nn}ê±´ â†’ NaN ì „í™˜")

    for c in iqr_cols:
        if c not in df.columns: 
            continue
        mask = detect_outliers_rolling_iqr(df[c].astype(float), window=iqr_window, k=iqr_k, past_only=past_only)
        n = int(mask.sum())
        if n:
        #     df.loc[mask, c] = np.nan
        # print(f" - {c}: ë¡¤ë§-IQR ì´ìƒì¹˜ {n}ê±´ â†’ NaN")
        # if n:
            # ğŸ” ì´ìƒì¹˜ ì¶œë ¥
            outliers = df.loc[mask, c]
            print(f"\n[IQR ì´ìƒì¹˜] {c}: {n}ê±´")
            print(outliers)  # ì²˜ìŒ 20ê±´ë§Œ ë³´ê¸°
            df.loc[mask, c] = np.nan
        else:
            print(f" - {c}: ì´ìƒì¹˜ ì—†ìŒ")

    for c in mad_cols:
        if c not in df.columns: 
            continue
        mask = detect_outliers_rolling_mad(df[c].astype(float), window=mad_window, z=mad_z, past_only=past_only)
        n = int(mask.sum())
        if n:
        #     df.loc[mask, c] = np.nan
        # print(f" - {c}: ë¡¤ë§-MAD ì´ìƒì¹˜ {n}ê±´ â†’ NaN")
            outliers = df.loc[mask, c]
            print(f"\n[MAD ì´ìƒì¹˜] {c}: {n}ê±´")
            print(outliers)  # ì²˜ìŒ 20ê±´ë§Œ ë³´ê¸°
            df.loc[mask, c] = np.nan
        else:
            print(f" - {c}: ì´ìƒì¹˜ ì—†ìŒ")

    return df


# =========================
# 2) ê³ ê¸‰ ë³´ê°„ê¸° (fit/transform)
# =========================
def _rolling_median_fill(s: pd.Series, window=7, max_len=3):
    out = s.copy()
    isna = out.isna()
    if not isna.any():
        return out, 0
    grp = (isna.ne(isna.shift())).cumsum()
    filled = 0
    med = out.shift(1).rolling(window, min_periods=1).median()
    for _, idx in out[isna].groupby(grp[isna]).groups.items():
        if len(idx) <= max_len:
            out.loc[idx] = med.loc[idx]
            filled += len(idx)
    return out, filled

class ImputerAdvanced:
    """
    ë³€ìˆ˜ë³„ ê·œì¹™ ê¸°ë°˜ + ë‹¤ë‹¨ê³„ ëŒ€ì²´:
      (year, month, dow) â†’ (month, dow) â†’ (month)
    """
    def __init__(self, max_linear_gap_temp=7, max_gap_short_counts=3):
        self.max_linear_gap_temp  = max_linear_gap_temp
        self.max_gap_short_counts = max_gap_short_counts
        self.stats_y_m_d = None
        self.stats_m_d   = None
        self.stats_m     = None
        # ë³€ìˆ˜ íƒ€ì…
        self.smooth_cols = ["TempAvg_C"]
        self.zero_cols   = ["Precip_mm"]  # ê²°ì¸¡=0 ê°€ì •
        self.count_cols  = ["Vehicle_Max","Arrival_Num","Departure_Num",
                            "entry_count","exit_count",
                            "ë„ë¯¼_Arr","ë„ë¯¼_Dep","ë„ë¯¼_ì…ì°¨","ë„ë¯¼_ì¶œì°¨",
                            "ë„ë¯¼_Arrival","ë„ë¯¼_Departure"]
        self.ratio_cols  = ["LCC_ratio","FSC_ratio","DOM_Portion","INT_Portion"]
        self.delay_cols  = ["Late"]
        self.inter_cols  = ["Temp_Precip_Interaction"]

    def _sel(self, df, cols):  return [c for c in cols if c in df.columns]

    def fit(self, df: pd.DataFrame):
        tmp = df.copy()
        tmp["year"]  = tmp.index.year
        tmp["month"] = tmp.index.month
        tmp["dow"]   = tmp.index.weekday
        numeric = tmp.select_dtypes(include=[np.number]).columns
        self.stats_y_m_d = tmp.groupby(["year","month","dow"])[list(numeric)].median()
        self.stats_m_d   = tmp.groupby(["month","dow"])[list(numeric)].median()
        self.stats_m     = tmp.groupby(["month"])[list(numeric)].median()
        return self

    def _lookup(self, idx, col):
        for store, key in [(self.stats_y_m_d, (idx.year, idx.month, idx.weekday())),
                           (self.stats_m_d,   (idx.month, idx.weekday())),
                           (self.stats_m,     (idx.month))]:
            try:
                return store.loc[key, col]
            except KeyError:
                continue
        return np.nan

    def transform(self, df: pd.DataFrame, limit_to_index: pd.DatetimeIndex=None) -> pd.DataFrame:
        x = df.copy()
        scope = x.index if limit_to_index is None else x.index[x.index.isin(limit_to_index)]

        # ê¸´ ê²°ì¸¡ ê²½ê³ 
        print("\n[3/6] ê¸´ ê²°ì¸¡ ê²½ê³ (Top 5 by column)")
        for c in x.columns:
            if not pd.api.types.is_numeric_dtype(x[c]): 
                continue
            runs = consecutive_nan_runs(x.loc[scope, c])
            if len(runs) > 0 and max(runs) >= 30:
                runs_sorted = sorted(runs, reverse=True)[:5]
                print(f" - {c}: ìµœì¥ {max(runs_sorted)}ì¼ ì—°ì† NaN, ìƒìœ„ {runs_sorted}")

        # 1) zero cols
        for c in self._sel(x, self.zero_cols):
            mask = x[c].isna() & x.index.isin(scope)
            x.loc[mask, c] = 0.0

        # 2) smooth cols (Temp)
        for c in self._sel(x, self.smooth_cols):
            s = x[c].copy()
            s.loc[scope] = s.loc[scope].interpolate(method="time", limit=self.max_linear_gap_temp)
            still = s.isna() & s.index.isin(scope)
            for idx in s.index[still]:
                v = self._lookup(idx, c)
                if not pd.isna(v): s.loc[idx] = v
            x[c] = s

        # 3) count cols
        for c in self._sel(x, self.count_cols):
            s = x[c].copy()
            s_scope = s.copy(); s_scope.loc[~s.index.isin(scope)] = np.nan
            s_new, _ = _rolling_median_fill(s_scope, window=7, max_len=self.max_gap_short_counts)
            sel = s.isna() & (~s_new.isna()); s.loc[sel] = s_new.loc[sel]
            still = s.isna() & s.index.isin(scope)
            for idx in s.index[still]:
                v = self._lookup(idx, c)
                if not pd.isna(v): s.loc[idx] = v
            s = s.clip(lower=0)
            x[c] = s

        # 4) delay cols (Late)
        for c in self._sel(x, self.delay_cols):
            s = x[c].copy()
            still = s.isna() & s.index.isin(scope)
            for idx in s.index[still]:
                v = self._lookup(idx, c)
                if not pd.isna(v): s.loc[idx] = v
            ql, qu = s.quantile(0.005), s.quantile(0.995)
            s = s.clip(ql, qu)
            x[c] = s

        # 5) ratio cols
        for c in self._sel(x, self.ratio_cols):
            s = x[c].copy()
            s.loc[scope] = s.loc[scope].ffill(limit=7).bfill(limit=7)
            still = s.isna() & s.index.isin(scope)
            for idx in s.index[still]:
                v = self._lookup(idx, c)
                if not pd.isna(v): s.loc[idx] = v
            s = s.clip(0.0, 1.0)
            x[c] = s

        # ìŒ ì •ê·œí™”
        if all(col in x.columns for col in ["LCC_ratio","FSC_ratio"]):
            ssum = x["LCC_ratio"] + x["FSC_ratio"]
            over1 = (ssum > 1.0) & x.index.isin(scope)
            if over1.any():
                x.loc[over1, ["LCC_ratio","FSC_ratio"]] = x.loc[over1, ["LCC_ratio","FSC_ratio"]].div(ssum[over1], axis=0)
                print("  [ì •ê·œí™”] LCC+FSC > 1.0 í–‰ ì •ê·œí™”:", int(over1.sum()))
        if all(col in x.columns for col in ["DOM_Portion","INT_Portion"]):
            ssum = x["DOM_Portion"] + x["INT_Portion"]
            over1 = (ssum > 1.0) & x.index.isin(scope)
            if over1.any():
                x.loc[over1, ["DOM_Portion","INT_Portion"]] = x.loc[over1, ["DOM_Portion","INT_Portion"]].div(ssum[over1], axis=0)
                print("  [ì •ê·œí™”] DOM+INT > 1.0 í–‰ ì •ê·œí™”:", int(over1.sum()))

        # ìƒí˜¸ì‘ìš© ì¬ê³„ì‚°
        if ("Temp_Precip_Interaction" in x.columns) and all(col in x.columns for col in ["TempAvg_C","Precip_mm"]):
            x["Temp_Precip_Interaction"] = x["TempAvg_C"] * x["Precip_mm"]

        # ì”ì—¬ NaN ê²½ê³ 
        remain = x.select_dtypes(include=[np.number]).isna().sum()
        remain = remain[remain > 0].sort_values(ascending=False)
        if len(remain) > 0:
            print("\n  [ì£¼ì˜] ë³´ê°„ í›„ì—ë„ NaN ì”ì¡´(ìƒìœ„ 10ê°œ):")
            print(remain.head(10))

        return x


# =========================
# 3) ë‹¤ì¤‘ê³µì„ ì„± ì œê±°
# =========================
def remove_multicollinearity_numeric(df, target="Vehicle_Max", threshold=0.95):
    """
    1) ìŠ¤í˜ì…œ í˜ì–´ ìš°ì„  ì²˜ë¦¬: (LCC,FSC), (DOM,INT) ì¤‘ ë’¤ìª½ ì œê±°
    2) ìˆ«ìí˜• ìƒê´€ > threshold â†’ í‰ê· ìƒê´€ ë†’ì€ ìª½ ì œê±°
    """
    to_drop = []

    special_pairs = [("LCC_ratio","FSC_ratio"), ("DOM_Portion","INT_Portion")]
    for a, b in special_pairs:
        if a in df.columns and b in df.columns:
            to_drop.append(b)

    df2 = df.drop(columns=to_drop, errors="ignore")

    num = df2.select_dtypes(include=[np.number]).drop(columns=[target], errors="ignore")
    if num.shape[1] == 0:
        print("\n[4/6] ë‹¤ì¤‘ê³µì„ ì„±: ìˆ«ìí˜• í”¼ì²˜ ì—†ìŒ â†’ ìŠ¤í‚µ")
        return df2, to_drop

    corr = num.corr().abs()
    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))

    auto_drop = set()
    pairs = []
    for r in upper.index:
        for c in upper.columns:
            v = upper.loc[r, c]
            if pd.notna(v) and v > threshold:
                pairs.append((r, c, float(v)))
                m_r = upper.loc[r].mean()
                m_c = upper[c].mean()
                drop = r if m_r >= m_c else c
                if drop != target:
                    auto_drop.add(drop)

    print("\n[4/6] ë‹¤ì¤‘ê³µì„ ì„± ìš”ì•½")
    if pairs:
        print(f"- ìƒê´€ > {threshold} í”¼ì²˜ìŒ ì˜ˆì‹œ(ìµœëŒ€ 10ê°œ):")
        for (r,c,v) in sorted(pairs, key=lambda x: -x[2])[:10]:
            print(f"  Â· {r} â†” {c} = {v:.3f}")
    else:
        print("- ì„ê³„ ì´ˆê³¼ ìƒê´€ìŒ ì—†ìŒ")

    if to_drop:
        print(f"- ìŠ¤í˜ì…œ í˜ì–´ ì œê±°: {sorted(to_drop)}")
    if auto_drop:
        print(f"- ìƒê´€ ê¸°ë°˜ ìë™ ì œê±°: {sorted(auto_drop)}")

    df_final = df2.drop(columns=list(auto_drop), errors="ignore")
    return df_final, sorted(to_drop) + sorted(list(auto_drop))


# =========================
# 4) RF ê¸°ë°˜ feature selection
# =========================
def rf_feature_filter(df, target="Vehicle_Max",
                      train_end="2025-01-09",
                      importance_thresh=0.002, topn_show=20):
    train = df.loc[df.index <= pd.Timestamp(train_end)].copy()
    num_feats = [c for c in train.select_dtypes(include=[np.number]).columns if c not in [target,"time_idx"]]
    if not num_feats:
        print("\n[5/6] RF í•„í„°: ìˆ«ìí˜• í”¼ì²˜ ì—†ìŒ â†’ ìŠ¤í‚µ")
        return df, pd.Series(dtype=float), []

    X = train[num_feats].fillna(0.0)
    y = train[target].astype(float)

    rf = RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1)
    rf.fit(X, y)
    imps = pd.Series(rf.feature_importances_, index=num_feats).sort_values(ascending=False)

    low = imps[imps < importance_thresh].index.tolist()

    print("\n[5/6] RF Feature Importance")
    print("- Top ì¤‘ìš” í”¼ì²˜:")
    print(imps.head(topn_show).to_string())
    print(f"- ì¤‘ìš”ë„ < {importance_thresh:.4f} ë“œë¡­ ëŒ€ìƒ({len(low)}ê°œ): {sorted(low)}")

    df_new = df.drop(columns=low, errors="ignore")
    return df_new, imps, low


# =========================
# 5) ì „ì²´ íŒŒì´í”„ë¼ì¸
# =========================
def run_preprocess(input_path="output/jeju.csv",
                        output_path="output/jeju_preprocessed.csv",
                        start_date="2014-01-01",
                        end_date="2025-04-30",
                        use_lag_roll=False,
                        impute_only_until=None,         # ì˜ˆ: "2025-01-09" (ë””ì½”ë” NaN ìœ ì§€)
                        drop_external_pred_cols=True,   # ê±°ì˜ ì „ë¶€ NaN ì»¬ëŸ¼ ìë™ ë“œë¡­
                        nan_ratio_drop=0.90,            # ê²°ì¸¡ë¹„ìœ¨ â‰¥ 90%ë©´ ë“œë¡­
                        corr_threshold=0.95,
                        rf_train_end="2025-01-09",
                        rf_importance_thresh=0.002):
    print("[ì‹œì‘] ì œì£¼ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (23~25)")
    raw = pd.read_csv(input_path)
    print(f"- ì…ë ¥ ê²½ë¡œ: {input_path}")
    print(f"- ì›ë³¸ shape: {raw.shape}")

    # 1) ê¸°ë³¸ ì „ì²˜ë¦¬
    df = preprocess_basics(raw, start_date=start_date, end_date=end_date)
    print(f"- ê¸°ë³¸ ì „ì²˜ë¦¬ í›„ shape: {df.shape}")

    # 2) ì´ìƒì¹˜ ë§ˆìŠ¤í‚¹ (â†’ NaN)
    zero_cols = [c for c in ["Vehicle_Max","Arrival_Num","Departure_Num","entry_count","exit_count",
                             "ë„ë¯¼_Arr","ë„ë¯¼_Dep","ë„ë¯¼_ì…ì°¨","ë„ë¯¼_ì¶œì°¨",
                             "ë„ë¯¼_Arrival","ë„ë¯¼_Departure"] if c in df.columns]
    iqr_cols  = [c for c in ["Arrival_Num","Departure_Num","entry_count","exit_count","Late"] if c in df.columns]
    mad_cols  = [c for c in ["Arrival_Num","Departure_Num","entry_count","exit_count","Late","TempAvg_C"] if c in df.columns]

    df = mask_outliers(df,
                       zero_outlier_cols=zero_cols,        # 0/ìŒìˆ˜ëŠ” NaN ì²˜ë¦¬
                       iqr_cols=iqr_cols,                  # ë¡¤ë§-IQR ì´ìƒì¹˜ NaN
                       mad_cols=mad_cols,                  # ë¡¤ë§-MAD ì´ìƒì¹˜ NaN
                       iqr_window=35, iqr_k=2.5,
                       mad_window=35, mad_z=5.0,
                       past_only=True)

    # 3) ë³´ê°„ ì „ NaN ìš”ì•½
    num_cols = df.select_dtypes(include=[np.number]).columns
    na_before = df[num_cols].isna().sum()
    print("\n[3/6] ì´ìƒì¹˜ ë§ˆìŠ¤í‚¹ í›„(ë³´ê°„ ì „) ìˆ«ìí˜• NaN ê°œìˆ˜(ìƒìœ„ 12)")
    print(na_before[na_before > 0].sort_values(ascending=False).head(12))

    # 4) ê³ ê¸‰ ë³´ê°„
    imputer = ImputerAdvanced()
    imputer.fit(df)
    scope_index = None
    if impute_only_until is not None:
        until = pd.to_datetime(impute_only_until)
        scope_index = df.index[df.index <= until]
        print(f"- ë³´ê°„ ë²”ìœ„ ì œí•œ: {impute_only_until} ê¹Œì§€ë§Œ (ë””ì½”ë” êµ¬ê°„ NaN ìœ ì§€)")
    df_imp = imputer.transform(df, limit_to_index=scope_index)

    # 5) (ì˜µì…˜) ê±°ì˜ ì „ë¶€ NaN ì»¬ëŸ¼ ë“œë¡­ (ì™¸ë¶€ ì˜ˆì¸¡ì¹˜ ë“±)
    dropped_mostly_nan = []
    if drop_external_pred_cols:
        na_ratio = df_imp.isna().mean().sort_values(ascending=False)
        cand = na_ratio[na_ratio >= nan_ratio_drop].index.tolist()
        if cand:
            df_imp = df_imp.drop(columns=cand, errors="ignore")
            dropped_mostly_nan = cand
            print(f"\n[ì˜µì…˜] ê²°ì¸¡ë¹„ìœ¨ â‰¥ {nan_ratio_drop*100:.0f}% ì»¬ëŸ¼ ë“œë¡­: {sorted(cand)}")

    # 6) (ì˜µì…˜) ì•ˆì „ lag/rolling
    if use_lag_roll:
        df_imp = add_lag_and_roll_safe(df_imp, target="Vehicle_Max", lags=(1,7,14), roll_windows=(7,14))
        print("- ì•ˆì „ lag/rolling ì¶”ê°€: lag(1,7,14), roll_mean/std(7,14)")

    # 7) ë‹¤ì¤‘ê³µì„ ì„± ì œê±°
    df_mc, dropped_mc = remove_multicollinearity_numeric(df_imp, target="Vehicle_Max", threshold=corr_threshold)

    # 8) RF feature importance í•„í„°
    df_final, imps, dropped_rf = rf_feature_filter(df_mc, target="Vehicle_Max",
                                                   train_end=rf_train_end,
                                                   importance_thresh=rf_importance_thresh)

    # 9) ìµœì¢… NaN ìš”ì•½
    num_cols_after = df_final.select_dtypes(include=[np.number]).columns
    na_after = df_final[num_cols_after].isna().sum()
    print("\n[6/6] ì „ì²˜ë¦¬-ìµœì¢… ìˆ«ìí˜• NaN ê°œìˆ˜(ìƒìœ„ 12)")
    print(na_after[na_after > 0].sort_values(ascending=False).head(12))

    # 10) ìƒ˜í”Œ ì¶œë ¥
    show_cols = ["Vehicle_Max","Arrival_Num","Departure_Num","TempAvg_C","Precip_mm",
                 "LCC_ratio","FSC_ratio","DOM_Portion","INT_Portion",
                 "Late","entry_count","exit_count"]
    show_cols = [c for c in show_cols if c in df_final.columns]
    print("\n[ìƒ˜í”Œ] head(5)")
    if show_cols:
        print(df_final.head(5)[show_cols].to_string())
        print("\n[ìƒ˜í”Œ] tail(5)")
        print(df_final.tail(5)[show_cols].to_string())
    else:
        print("í‘œì‹œí•  ìˆ˜ì¹˜í˜• ìƒ˜í”Œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 11) ë“œë¡­ ìš”ì•½
    print("\n[ë“œë¡­ ìš”ì•½]")
    if dropped_mostly_nan:
        print(" - ê²°ì¸¡ë¹„ìœ¨ ê¸°ì¤€:", sorted(dropped_mostly_nan))
    if dropped_mc:
        print(" - ë‹¤ì¤‘ê³µì„ ì„± ê¸°ì¤€:", sorted(dropped_mc))
    if dropped_rf:
        print(" - RF ì¤‘ìš”ë„ ê¸°ì¤€:", sorted(dropped_rf))
    if not (dropped_mostly_nan or dropped_mc or dropped_rf):
        print(" - ë“œë¡­ëœ ì»¬ëŸ¼ ì—†ìŒ")

    print("\n[ìµœì¢… ì»¬ëŸ¼ ëª©ë¡]")
    cols = df_final.columns.tolist()
    print(f"- ìµœì¢… ì»¬ëŸ¼ ìˆ˜: {len(cols)}")
    print("Â· ì»¬ëŸ¼ë“¤:")
    for i, c in enumerate(cols, 1):
        print(f"  {i:2d}. {c}")
        
    # 12) ì €ì¥
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df_final.reset_index().to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"\n[ì™„ë£Œ] ì €ì¥: {output_path}")
    print("[ë] ì œì£¼ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (23~25)")

    return df_final


# =========================
# ì‹¤í–‰ ì˜ˆì‹œ
# =========================
# if __name__ == "__main__":
#     input_dir = "output"
#     output_dir = "output"

#     # input_dir ì•ˆì˜ csv íŒŒì¼ë“¤ ìˆœíšŒ
#     for fname in os.listdir(input_dir):
#         if not fname.endswith(".csv"):
#             continue
#         if fname.endswith("_preprocessed.csv"):
#             continue   # ì´ë¯¸ ì „ì²˜ë¦¬ëœ ê±´ ìŠ¤í‚µ

#         input_path = os.path.join(input_dir, fname)

#         # í™•ì¥ì ì œê±° í›„ ë’¤ì— _preprocessed ë¶™ì´ê¸°
#         base, _ = os.path.splitext(fname)
#         output_path = os.path.join(output_dir, f"{base}_preprocessed.csv")

#         print(f"\n=== {fname} ì²˜ë¦¬ ì‹œì‘ ===")
#         _ = run_preprocess(
#             input_path=input_path,
#             output_path=output_path,
#             start_date="2014-01-01",
#             end_date="2025-04-30",
#             use_lag_roll=False,
#             impute_only_until=None,
#             drop_external_pred_cols=True,
#             nan_ratio_drop=0.90,
#             corr_threshold=0.95,
#             rf_train_end="2025-01-09",
#             rf_importance_thresh=0.002
#         )
#         print(f"=== {fname} ì²˜ë¦¬ ì™„ë£Œ ===\n")
if __name__ == "__main__": 
    _ = run_preprocess(input_path="output/jeju.csv", # â† í•„ìš” ì‹œ ê²½ë¡œ ë³€ê²½ 
                            output_path="output/jeju_preprocessed.csv", 
                            start_date="2014-01-01", 
                            end_date="2025-04-30", 
                            use_lag_roll=False, # Trueë¡œ ì¼œë©´ ì•ˆì „ lag/rolling ìƒì„± 
                            impute_only_until=None, # ì˜ˆ: "2025-01-09" (ë””ì½”ë” NaN ìœ ì§€) 
                            drop_external_pred_cols=True, # ê±°ì˜ ì „ë¶€ NaNì¸ ì»¬ëŸ¼ ìë™ ë“œë¡­ 
                            nan_ratio_drop=0.90, # 90% ì´ìƒ NaNì´ë©´ ë“œë¡­ 
                            corr_threshold=0.95, # ë‹¤ì¤‘ê³µì„ ì„± ì„ê³„ 
                            rf_train_end="2025-01-09", # RF í•™ìŠµ ìƒí•œ(ëˆ„ì¶œ ë°©ì§€) 
                            rf_importance_thresh=0.002 # RF ì¤‘ìš”ë„ í•˜í•œ(0.2%) 
                            )
